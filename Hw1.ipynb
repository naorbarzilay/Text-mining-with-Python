{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a6051ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import time\n",
    "#https://stackoverflow.com/questions/1557571/how-do-i-get-time-of-a-python-programs-execution\n",
    "\n",
    "\n",
    "# decision tree\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "\n",
    "# adaboost\n",
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor\n",
    "\n",
    "# gradient boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "# model building\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# model evaluation\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from fast_ml.model_development import train_valid_test_split\n",
    "# set seed for reproducible results\n",
    "RSEED = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fed580",
   "metadata": {},
   "source": [
    "# פונקציות עזר"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8abdb20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# פונקציה עבור ערכים מספרים - ממיינת את כל ערכי התצפיות ומחזירה מערך עם ממוצע בין כל זוג\n",
    "#https://stackoverflow.com/questions/20527563/average-of-two-consecutive-elements-in-the-list-in-python\n",
    "def cal_follow_avg(values):\n",
    "    sorted_numbers = sorted(values)\n",
    "    res= [(sorted_numbers[i]+sorted_numbers[i+1])/2 for i in range(0, len(sorted_numbers)-1, 1)]\n",
    "    return res\n",
    "\n",
    "# פונקציה שמחשבת ג'יני\n",
    "def cumpute_gini(parent, l_child, r_child):\n",
    "        weight_l = len(l_child) / len(parent)\n",
    "        weight_r = len(r_child) / len(parent)\n",
    "        gini = (weight_l*cumpute_gini_help(l_child) + weight_r*cumpute_gini_help(r_child))\n",
    "        return gini\n",
    "\n",
    "# פונקציה עזר לחישוב הג'יני\n",
    "def cumpute_gini_help(y):       \n",
    "        feature_value = np.unique(y) \n",
    "        gini = 0\n",
    "        for val in feature_value:\n",
    "            count = 0\n",
    "            for y_val in y:\n",
    "                if(y_val==val):\n",
    "                    count+=1\n",
    "            score= count/len(y) \n",
    "            gini += score**2\n",
    "        return 1 - gini\n",
    "\n",
    "# פונקציה שמגרילה מספרים ויוצרת מסד נתונים חדש מהתצפיות - עבור מימוש האדאבוסט - Adaboost\n",
    "def create_new_dataset(dataset,w):\n",
    "        ranges= []\n",
    "        newdataset= []\n",
    "        start=0;\n",
    "        for weight in w:\n",
    "            ranges.append((start,start+weight)) #  יוצר רשימה עם טווחים, לדוגמה: (0.5,0.7)(0,0.5)\n",
    "            start+=weight\n",
    "        for i in range(len(dataset)):\n",
    "            n = random.random() # מגריל מספר בין 0 ל-1\n",
    "            for r in ranges:\n",
    "                if n<r[1] and n>=r[0]: # בודק באיזה תא המספר\n",
    "                    index=ranges.index(r)\n",
    "            newdataset.append(dataset[index]) #מוסיף למסד נתונים חדש את הרשימה שהוגרלה\n",
    "        return np.array(newdataset)\n",
    "    \n",
    "def cumpute_mse(parent, left, right):\n",
    "    #https://medium.com/analytics-vidhya/regression-trees-decision-tree-for-regression-machine-learning-e4d7525d8047\n",
    "    weight_l = len(left) / len(parent)\n",
    "    weight_r = len(right) / len(parent)\n",
    "    mse = (weight_l*np.var(left) + weight_r*np.var(right)) # np.var - מחשב שונות\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdb0c21",
   "metadata": {},
   "source": [
    "<h1 style='color:red'>Section A:</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d564e78",
   "metadata": {},
   "source": [
    "# Classify Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e8badee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self,feature_index=None,feature_name=None, threshold=None, left=None, right=None, gini=None, value=None,cur_depth=None):     \n",
    "        # for decision node\n",
    "        self.feature_index=feature_index # אינדקס הפיצ'ר\n",
    "        self.feature_name = feature_name # שם הפיצר\n",
    "        self.threshold = threshold # ערך הפיצ'ר בעץ\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.gini = gini\n",
    "        self.depth= cur_depth\n",
    "        self.value = value  # בעלים יכיל את התוצאה הסופית"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "300f430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name=['availability','bedrooms','total_sqft','bath','balcony','ranked','price in rupees']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8984063",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyDecisionTree():\n",
    "    def __init__(self,min_sample_split=3, max_depth=5):   \n",
    "        self.root = None\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def build_tree(self, dataset, curr_depth=0):          \n",
    "        X, Y = dataset[:,:-1], dataset[:,-1]\n",
    "        # X - get all the values without area_type [1.0 3.0 1550.0 ... 3.0 261.0 7000000.0]\n",
    "        # Y - get only area_type ['B' 'B' 'B' 'B' 'B' 'B' 'B' 'P' ]\n",
    "        \n",
    "        num_of_Observ , num_features = np.shape(X) # מקבלים את מס' התצפיות והעמודות בהתאמה\n",
    "        \n",
    "        #מס' התצפיות גדול ממינימום הפיצול שהגדרנו וגם עומק העץ קטן מהעומק המקסימלי\n",
    "        if num_of_Observ>=self.min_sample_split and curr_depth<=self.max_depth: # תנאי עצירה לפונקציה\n",
    "            best_split = self.get_best_split(dataset, num_of_Observ, num_features) # קורא לפונקציה שמחזירה את הפיצול הטוב ביותר\n",
    "            if best_split[5]>0: # האינדקס של הגי'ני\n",
    "                left = self.build_tree(best_split[3], curr_depth+1)\n",
    "                right= self.build_tree(best_split[4], curr_depth+1)\n",
    "                return Node(best_split[1],best_split[0], best_split[2],left, right, best_split[5], curr_depth)\n",
    "        \n",
    "        # מחזיר את ההחלטה\n",
    "        leaf_value = self.calculate_leaf_value(Y) # מחזירה את ההחלטה\n",
    "        return Node(value=leaf_value) # יוצר עלה עם ערך ההחלטה\n",
    "    \n",
    "    def calculate_leaf_value(self, Y): #פונקציה המחזירה את ההחלטה הסופית  \n",
    "        Y = list(Y)\n",
    "        return max(Y, key=Y.count) # מחזירה את האלמנט שמופיע הכי הרבה פעמים ברשימה שנמצאת בעלה\n",
    "    \n",
    "    def get_best_split(self, dataset, num_of_Observ, num_features):\n",
    "        best_split = ()\n",
    "        min_gini = float(\"inf\")\n",
    "        \n",
    "        # לולאה שעוברת על כל הפיצ'רים ומחפשת את הפיצול הטוב ביותר בעזרת ג'יני\n",
    "        for feature_index in range(num_features):\n",
    "            feature_values = dataset[:, feature_index] # מקבל רק הערכים של אותו פיצ'ר\n",
    "            possible_thresholds=cal_follow_avg(np.unique(feature_values)) # מחשב ממוצע לערכים מספריים - כל הערכים הם מספריים ולכן לא עשינו בדיקה\n",
    "            for threshold in possible_thresholds: \n",
    "                dataset_left, dataset_right = self.split(dataset, feature_index, threshold) #מפצל \n",
    "                # בודק שהצומת מלאה - בן שמאלי וימני\n",
    "                if len(dataset_left)>0 and len(dataset_right)>0:\n",
    "                    root, left, right = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1] # y=parent\n",
    "                    curr_gini = cumpute_gini(root, left, right) # מחשב ג'יני\n",
    "                    # מעדכן את הפיצול הטוב ביותר במידה ומצאנו ערך ג'יני קטן יותר\n",
    "                    if curr_gini<min_gini: \n",
    "                        # ערכי הצומת הטובה ביותר\n",
    "                        best_split=(feature_name[feature_index],feature_index,threshold,dataset_left,dataset_right,curr_gini)\n",
    "                        min_gini = curr_gini\n",
    "        return best_split\n",
    "      \n",
    "    def split(self, dataset, feature_index, threshold):    \n",
    "        # מפצלת את הערכים לפי הגבול\n",
    "        dataset_left=[]\n",
    "        dataset_right=[]\n",
    "        for row in dataset:\n",
    "            if row[feature_index]<=threshold:\n",
    "                dataset_left.append(row)\n",
    "            else:\n",
    "                dataset_right.append(row)\n",
    "        return np.array(dataset_left), np.array(dataset_right)\n",
    "    \n",
    "    def fit(self, X, Y,min_leaf = 5):\n",
    "        dataset = np.concatenate((X, Y), axis=1) # מחבר את שני המערכים X,Y\n",
    "        self.root = self.build_tree(dataset)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        #עוברת על כל שורה ומחזירה את החיזוי\n",
    "        preditions=[]\n",
    "        for row in X:\n",
    "            preditions.append(self.make_prediction(row, self.root))\n",
    "        return preditions\n",
    "    \n",
    "    def make_prediction(self, row, tree):      \n",
    "        if tree.value=='B' or tree.value =='P': return tree.value #תנאי עצירה - הגענו לעלה\n",
    "        feature_val = row[tree.feature_index]\n",
    "        if feature_val<=tree.threshold:\n",
    "            return self.make_prediction(row, tree.left)\n",
    "        else:\n",
    "            return self.make_prediction(row, tree.right)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5744b2",
   "metadata": {},
   "source": [
    "# Regresssion Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e9f1993",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, feature_index=None,feature_name=None, threshold=None, left=None, right=None, mse=None, value=None):\n",
    "        self.feature_name=feature_name\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.mse = mse\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17545dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name=['area_type','availability','bedrooms','total_sqft','bath','balcony','ranked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "80fc0669",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegresssionTree():\n",
    "    def __init__(self,min_sample_split=3, max_depth=5):   \n",
    "        self.root = None\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def build_tree(self, dataset, curr_depth=0):          \n",
    "        X, Y = dataset[:,:-1], dataset[:,-1]\n",
    "        \n",
    "        # X - get all the values without area_type [1.0 3.0 1550.0 ... 3.0 261.0 7000000.0]\n",
    "        # Y - get only area_type ['B' 'B' 'B' 'B' 'B' 'B' 'B' 'P' ]\n",
    "        \n",
    "        num_of_Observ , num_features = np.shape(X) # מקבלים את מס' התצפיות והעמודות בהתאמה\n",
    "        \n",
    "        #מס' התצפיות גדול ממינימום הפיצול שהגדרנו וגם שהעומק לא עבר את העומק המקסימלי שהגדרנו \n",
    "        if num_of_Observ>=self.min_sample_split and curr_depth<=self.max_depth: # תנאי עצירה \n",
    "            best_split = self.get_best_split(dataset, num_of_Observ, num_features) # מחזירה את הפיצול הטוב ביותר\n",
    "            if best_split[5]>0: # האינדקס של ה-MSE\n",
    "                left = self.build_tree(best_split[3], curr_depth+1)\n",
    "                right= self.build_tree(best_split[4], curr_depth+1)\n",
    "                return Node(best_split[1],best_split[0], best_split[2],left, right, best_split[5], curr_depth)\n",
    " \n",
    "        leaf_value = self.calculate_leaf_value(Y) #מחשב את הערך הסופי בעלה (ממוצע)\n",
    "        return Node(value=leaf_value) # יוצר את העלה ומחזיר אותו\n",
    "        \n",
    "    def calculate_leaf_value(self, Y): #פונקציה המחזירה את ההחלטה הסופית  \n",
    "        return int(np.mean(Y)) # מחזיר את הממוצע של התצפיות\n",
    "    \n",
    "    def get_best_split(self, dataset, num_of_Observ, num_features):\n",
    "        best_split = ()\n",
    "        min_mse = float(\"inf\")\n",
    "        \n",
    "        # לולאה שעוברת על כל הפיצ'רים ומחפשת את הפיצול הטוב ביותר בעזרת MSE\n",
    "        for feature_index in range(num_features):\n",
    "            feature_values = dataset[:, feature_index]\n",
    "            possible_thresholds = np.unique(feature_values) # מוריד כפילויות, נותן את הערכים שאפשר להיות לאותה שאלה (פיצר)\n",
    "            # בודק שאנחנו לא בודקים את ה-type area \n",
    "            if(feature_index!=0):\n",
    "                possible_thresholds=cal_follow_avg(possible_thresholds)         \n",
    "            for threshold in possible_thresholds:\n",
    "                dataset_left, dataset_right = self.split(dataset, feature_index, threshold) # פיצול לפי ערכים מספריים\n",
    "                     # בודק שהצומת מלאה\n",
    "                if len(dataset_left)>0 and len(dataset_right)>0:\n",
    "                    root, left, right = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1] # y= parent\n",
    "                    # compute mse\n",
    "                    curr_mse = cumpute_mse(root, left, right)\n",
    "                    # במידה ומצאנו נמוך יותר אז מחליפים את הצומת\n",
    "                    if curr_mse<min_mse:\n",
    "                        best_split=(feature_name[feature_index],feature_index,threshold,dataset_left,dataset_right,curr_mse)\n",
    "                        min_mse = curr_mse  \n",
    "        return best_split\n",
    "    \n",
    "    def split(self, dataset, feature_index, threshold):    \n",
    "        # מפצלת את הערכים לפי הגבול\n",
    "        dataset_left=[]\n",
    "        dataset_right=[]\n",
    "        for row in dataset:\n",
    "            if row[feature_index]<=threshold:\n",
    "                dataset_left.append(row)\n",
    "            else:\n",
    "                dataset_right.append(row)\n",
    "        return np.array(dataset_left), np.array(dataset_right)\n",
    "      \n",
    "    def fit(self, X, Y,min_leaf = 5):\n",
    "        dataset = np.concatenate((X, Y), axis=1) # מחבר את שני המערכים X,Y\n",
    "        self.root = self.build_tree(dataset)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        #עוברת על כל שורה ומחזירה את החיזוי\n",
    "        preditions=[]\n",
    "        for row in X:\n",
    "            preditions.append(self.make_prediction(row, self.root))\n",
    "        return preditions\n",
    "    \n",
    "    def make_prediction(self, row, tree):  \n",
    "        if(tree.left is None and tree.right is None): \n",
    "            return tree.value # תנאי עצירה - כאשר הגענו לעלה נחזיר את ערכו\n",
    "        feature_val = row[tree.feature_index]\n",
    "        if feature_val<=tree.threshold:\n",
    "            return self.make_prediction(row, tree.left)\n",
    "        else:\n",
    "            return self.make_prediction(row, tree.right)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "750a9e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaboost:\n",
    "    def __init__(self, min_interaction=10):\n",
    "        self.min_interaction = min_interaction\n",
    "        self.stumps = [] # מערך של כל הסטאמפ שיצרנו\n",
    "        self.alphas = [] # מערך של האלפאות בהתמאה להסטאמפ\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        #נשמור את המסד נתונים המקורי ואיתו ניצור כל פעם מסד חדש\n",
    "        original_dataset = np.concatenate((X, y), axis=1)\n",
    "        for _ in range(self.min_interaction): # עובר לי כמות האינטרקציות שקיבלנו\n",
    "            num_of_Observ , num_features = X.shape\n",
    "            w = np.full(num_of_Observ, (1 / num_of_Observ)) # מאתחל מערך עם משקלים בגודל של - 1 חלקי כמות הכוללת\n",
    "            wrong_indexs=[] # יאחסן את מיקומי הרשומות הלא נכונות\n",
    "            single_stump = ClassifyDecisionTree(min_sample_split=3, max_depth=1) # עץ החלטה עם עומק 1 \n",
    "            single_stump.fit(X,y) # בונה את הסטאמפ\n",
    "            sum_error=0 #סכום השגיאה הכוללת\n",
    "            count=0 #עוזר לנו לשמור את הרשומות הלא נכונות\n",
    "            for val in X.iloc[:, single_stump.root.feature_index].values.astype(int):\n",
    "                if(val>single_stump.root.threshold):\n",
    "                    if(y.values[count]=='B'): # אנחנו מצפים לקבל P\n",
    "                        sum_error+=w[0] # המשקל שווה לכל הרשומות בשלב זה\n",
    "                        wrong_indexs.append(count) # מוסיף את הטעות למערך\n",
    "                else:\n",
    "                    if(y.values[count]=='P'):\n",
    "                        sum_error+=w[0]\n",
    "                        wrong_indexs.append(count) # מוסיף את הטעות למערך\n",
    "                count+=1\n",
    "\n",
    "            # מחשב את אלפא\n",
    "            alpha = 0.5 * np.log((1.0 - sum_error) / (sum_error))\n",
    "            \n",
    "            # עובר על מערך המשקולות והשגיאות , אם רשומה נחשבת כטעות אז נעלה את ערכה כמו שלמדנו בתרגול, אחרת נקטין את ערכה\n",
    "            for index in range(len(w)):\n",
    "                if(index in wrong_indexs):\n",
    "                    w[index]*= np.exp(alpha) # רשומה שהתגלתה כטעות\n",
    "                else:\n",
    "                    w[index]*= np.exp(-alpha) # רשומה נכונה\n",
    "                \n",
    "            # נרמול המשקלים\n",
    "            w /= np.sum(w)\n",
    "            #dataset = np.concatenate((X, y), axis=1)\n",
    "            new_dataset=create_new_dataset(original_dataset,w) # יצירת מסד נתונים חדש ע\"י הגרלה\n",
    "            X= new_dataset[:,:-1] # מקבל את הערכים חוץ - area_type\n",
    "            y=new_dataset[:,-1] # מקבל רק את ערכי ה-area_type\n",
    "            # מחזיר אותם בחזרה למבנה נתונים מסוג DF\n",
    "            X= pd.DataFrame(X,columns = ['availability','bedrooms','total_sqft','bath','balcony','ranked','price in rupees'])\n",
    "            y = pd.DataFrame(y, columns = [\"area_type\"])\n",
    "            \n",
    "            # נשמור את הסטאמפ והאלפא במערך\n",
    "            self.stumps.append(single_stump)\n",
    "            self.alphas.append(alpha)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions=[]\n",
    "        self.alphas = [-num if num < 0 else num for num in self.alphas] # הופך את כלל האלפאות לחיוביות (במידה ויש כאלו)\n",
    "        for row in X: #עובר על כל רשומה\n",
    "            weak_sum = 0.0\n",
    "            #https://stackoverflow.com/questions/49783594/for-loop-and-zip-in-python\n",
    "            for stmp,alpha in zip(self.stumps,self.alphas):\n",
    "                if(stmp.make_prediction(row,stmp.root)=='B'): # B is the postive value\n",
    "                    weak_sum += alpha\n",
    "                else:\n",
    "                    weak_sum -=alpha\n",
    "            #If the same is Postive value - the predict is B, else P\n",
    "            if(weak_sum>0):\n",
    "                predictions.append('B')\n",
    "            else:\n",
    "                predictions.append('P')\n",
    "        return predictions\n",
    "               \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bee218",
   "metadata": {},
   "source": [
    "# <h1 style='color:red'>Section B:</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55055be2",
   "metadata": {},
   "source": [
    "# Classify Decision Tree checks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decf4fa1",
   "metadata": {},
   "source": [
    "# Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e30f7b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "Y_train=df.iloc[1:8040]\n",
    "Y_train.drop(['availability', 'Unnamed: 0','bedrooms','total_sqft','bath','balcony','ranked','price in rupees'], axis=1, inplace=True)\n",
    "\n",
    "X_train=df.iloc[1:8040]\n",
    "X_train.drop(['area_type', 'Unnamed: 0'], axis=1, inplace=True)\n",
    "\n",
    "Y_vald=df.iloc[8041:10050]\n",
    "Y_vald.drop(['availability', 'Unnamed: 0','bedrooms','total_sqft','bath','balcony','ranked','price in rupees'], axis=1, inplace=True)\n",
    "\n",
    "X_vald=df.iloc[8041:10050]\n",
    "X_vald.drop(['area_type', 'Unnamed: 0'], axis=1, inplace=True)\n",
    "\n",
    "Y_test=df.iloc[10051:12563]\n",
    "Y_test.drop(['availability', 'Unnamed: 0','bedrooms','total_sqft','bath','balcony','ranked','price in rupees'], axis=1, inplace=True)\n",
    "\n",
    "X_test=df.iloc[10051:12563,2:].values\n",
    "X_test=X_test.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1756d64",
   "metadata": {},
   "source": [
    "הוספנו זמן ריצה רק להרצה הכי טובה (בדקנו לפני ההגשה)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73f749dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken: 00:00:12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a03ad_row3_col0, #T_a03ad_row3_col1, #T_a03ad_row3_col2, #T_a03ad_row3_col3, #T_a03ad_row3_col4 {\n",
       "  background: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a03ad_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >min_sample_split</th>\n",
       "      <th class=\"col_heading level0 col1\" >max_depth</th>\n",
       "      <th class=\"col_heading level0 col2\" >Accuracy</th>\n",
       "      <th class=\"col_heading level0 col3\" >Sensitivity</th>\n",
       "      <th class=\"col_heading level0 col4\" >Specificity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a03ad_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_a03ad_row0_col0\" class=\"data row0 col0\" >3.000000</td>\n",
       "      <td id=\"T_a03ad_row0_col1\" class=\"data row0 col1\" >3.000000</td>\n",
       "      <td id=\"T_a03ad_row0_col2\" class=\"data row0 col2\" >0.898089</td>\n",
       "      <td id=\"T_a03ad_row0_col3\" class=\"data row0 col3\" >0.637143</td>\n",
       "      <td id=\"T_a03ad_row0_col4\" class=\"data row0 col4\" >0.940333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a03ad_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_a03ad_row1_col0\" class=\"data row1 col0\" >2.000000</td>\n",
       "      <td id=\"T_a03ad_row1_col1\" class=\"data row1 col1\" >4.000000</td>\n",
       "      <td id=\"T_a03ad_row1_col2\" class=\"data row1 col2\" >0.892516</td>\n",
       "      <td id=\"T_a03ad_row1_col3\" class=\"data row1 col3\" >0.634286</td>\n",
       "      <td id=\"T_a03ad_row1_col4\" class=\"data row1 col4\" >0.934320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a03ad_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_a03ad_row2_col0\" class=\"data row2 col0\" >3.000000</td>\n",
       "      <td id=\"T_a03ad_row2_col1\" class=\"data row2 col1\" >5.000000</td>\n",
       "      <td id=\"T_a03ad_row2_col2\" class=\"data row2 col2\" >0.901274</td>\n",
       "      <td id=\"T_a03ad_row2_col3\" class=\"data row2 col3\" >0.622857</td>\n",
       "      <td id=\"T_a03ad_row2_col4\" class=\"data row2 col4\" >0.946346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a03ad_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_a03ad_row3_col0\" class=\"data row3 col0\" >4.000000</td>\n",
       "      <td id=\"T_a03ad_row3_col1\" class=\"data row3 col1\" >6.000000</td>\n",
       "      <td id=\"T_a03ad_row3_col2\" class=\"data row3 col2\" >0.902468</td>\n",
       "      <td id=\"T_a03ad_row3_col3\" class=\"data row3 col3\" >0.620000</td>\n",
       "      <td id=\"T_a03ad_row3_col4\" class=\"data row3 col4\" >0.948196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1d53f3b8670>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maindf=pd.DataFrame().T\n",
    "\n",
    "classifier = ClassifyDecisionTree(min_sample_split=3, max_depth=3)\n",
    "classifier.fit(X_vald,Y_vald)\n",
    "\n",
    "Y_pred = classifier.predict(X_test) \n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(Y_test, Y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "accuracy= (tp + tn)/(tp + tn + fp + fn)\n",
    "sensitivity= tp / (tp + fn )\n",
    "\n",
    "df = pd.DataFrame([3,3,accuracy,sensitivity,specificity]).T\n",
    "df = df.rename(columns={0:'min_sample_split',1:'max_depth',2: 'Accuracy',3:'Sensitivity',4:'Specificity'})\n",
    "\n",
    "maindf = maindf.append(df,ignore_index=True)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "classifier = ClassifyDecisionTree(min_sample_split=2, max_depth=4)\n",
    "classifier.fit(X_vald,Y_vald)\n",
    "\n",
    "Y_pred = classifier.predict(X_test) \n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(Y_test, Y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "accuracy= (tp + tn)/(tp + tn + fp + fn)\n",
    "sensitivity= tp / (tp + fn )\n",
    "\n",
    "df = pd.DataFrame([2,4,accuracy,sensitivity,specificity]).T\n",
    "df = df.rename(columns={0:'min_sample_split',1:'max_depth',2: 'Accuracy',3:'Sensitivity',4:'Specificity'})\n",
    "maindf=maindf.append(df,ignore_index=True)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "classifier = ClassifyDecisionTree(min_sample_split=3, max_depth=5)\n",
    "classifier.fit(X_vald,Y_vald)\n",
    "\n",
    "Y_pred = classifier.predict(X_test) \n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(Y_test, Y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "accuracy= (tp + tn)/(tp + tn + fp + fn)\n",
    "sensitivity= tp / (tp + fn )\n",
    "\n",
    "df = pd.DataFrame([3,5,accuracy,sensitivity,specificity]).T\n",
    "df = df.rename(columns={0:'min_sample_split',1:'max_depth',2: 'Accuracy',3:'Sensitivity',4:'Specificity'})\n",
    "\n",
    "maindf=maindf.append(df,ignore_index=True)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "start_time = time.time()\n",
    "\n",
    "classifier = ClassifyDecisionTree(min_sample_split=4, max_depth=6)\n",
    "classifier.fit(X_vald,Y_vald)\n",
    "Y_pred = classifier.predict(X_test) \n",
    "\n",
    "seconds = time.time() - start_time\n",
    "print('Time Taken:', time.strftime(\"%H:%M:%S\",time.gmtime(seconds)))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(Y_test, Y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "accuracy= (tp + tn)/(tp + tn + fp + fn)\n",
    "sensitivity= tp / (tp + fn )\n",
    "\n",
    "df = pd.DataFrame([4,6,accuracy,sensitivity,specificity]).T\n",
    "df = df.rename(columns={0:'min_sample_split',1:'max_depth',2: 'Accuracy',3:'Sensitivity',4:'Specificity'})\n",
    "\n",
    "\n",
    "maindf=maindf.append(df,ignore_index=True)\n",
    "maindf.style.apply(lambda x: ['background: lightgreen' if x.name in [3] else '' for i in x], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9fbf56",
   "metadata": {},
   "source": [
    "# Regresssion Tree checks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fe61b4",
   "metadata": {},
   "source": [
    "# Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e5bba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df = pd.get_dummies(df, columns=['area_type'], drop_first=True)\n",
    "df = df.astype({\"balcony\":\"int\",\"availability\":\"int\",\"bedrooms\":\"int\",\"total_sqft\":\"int\",\"bath\":\"int\",\"price in rupees\":\"int\"})\n",
    "\n",
    "Y_train=df.iloc[1:8040]\n",
    "Y_train.drop(['availability', 'Unnamed: 0','bedrooms','total_sqft','bath','balcony','ranked','area_type_P'], axis=1, inplace=True)\n",
    "\n",
    "X_train=df.iloc[1:8040]\n",
    "X_train.drop(['price in rupees', 'Unnamed: 0'], axis=1, inplace=True)\n",
    "\n",
    "Y_vald=df.iloc[8041:10051]\n",
    "Y_vald.drop(['availability', 'Unnamed: 0','bedrooms','total_sqft','bath','balcony','ranked','area_type_P'], axis=1, inplace=True)\n",
    "\n",
    "X_vald=df.iloc[8041:10051]\n",
    "X_vald.drop(['price in rupees', 'Unnamed: 0'], axis=1, inplace=True)\n",
    "\n",
    "Y_test=df.iloc[10051:12563]\n",
    "Y_test.drop(['availability', 'Unnamed: 0','bedrooms','total_sqft','bath','balcony','ranked','area_type_P'], axis=1, inplace=True)\n",
    "\n",
    "X_test=df.iloc[10051:12563]\n",
    "X_test.drop(['price in rupees', 'Unnamed: 0'], axis=1, inplace=True)\n",
    "X_test=df.iloc[10051:12563,1:-1].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4fcf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://community.dataquest.io/t/why-is-scoring-equal-to-neg-mean-squared-error/547283\n",
    "\n",
    "maindf=pd.DataFrame().T\n",
    "\n",
    "regressor = RegresssionTree(min_sample_split=3, max_depth=3)\n",
    "regressor.fit(X_vald,Y_vald)\n",
    "\n",
    "Y_pred = regressor.predict(X_test) \n",
    "MSE = round(mean_squared_error(Y_test, Y_pred), 1)\n",
    "mse=abs(MSE)\n",
    "\n",
    "df = pd.DataFrame([3,3,mse]).T\n",
    "df = df.rename(columns={0:'min_sample_split',1:'max_depth',2: 'MSE'})\n",
    "maindf = maindf.append(df,ignore_index=True)\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "regressor = RegresssionTree(min_sample_split=3, max_depth=4)\n",
    "regressor.fit(X_vald,Y_vald)\n",
    "\n",
    "Y_pred = regressor.predict(X_test) \n",
    "\n",
    "\n",
    "MSE = round(mean_squared_error(Y_test, Y_pred), 1)\n",
    "mse=abs(MSE)\n",
    "df = pd.DataFrame([3,4,mse]).T\n",
    "df = df.rename(columns={0:'min_sample_split',1:'max_depth',2: 'MSE'})\n",
    "maindf = maindf.append(df,ignore_index=True)\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "regressor = RegresssionTree(min_sample_split=3, max_depth=5)\n",
    "regressor.fit(X_vald,Y_vald)\n",
    "Y_pred = regressor.predict(X_test) \n",
    "\n",
    "MSE = round(mean_squared_error(Y_test, Y_pred), 1)\n",
    "mse=abs(MSE)\n",
    "df = pd.DataFrame([3,5,mse]).T\n",
    "df = df.rename(columns={0:'min_sample_split',1:'max_depth',2: 'MSE'})\n",
    "maindf = maindf.append(df,ignore_index=True)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "regressor = RegresssionTree(min_sample_split=3, max_depth=6)\n",
    "regressor.fit(X_vald,Y_vald)\n",
    "\n",
    "Y_pred = regressor.predict(X_test) \n",
    "\n",
    "MSE = round(mean_squared_error(Y_test, Y_pred), 1)\n",
    "mse = abs(MSE)\n",
    "\n",
    "df = pd.DataFrame([3,6,mse]).T\n",
    "df = df.rename(columns={0:'min_sample_split',1:'max_depth',2: 'MSE'})\n",
    "maindf = maindf.append(df,ignore_index=True)\n",
    "\n",
    "maindf.style.apply(lambda x: ['background: lightgreen' if x.name in [3] else '' for i in x], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e09de8",
   "metadata": {},
   "source": [
    "# Adaboost checks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80391d8e",
   "metadata": {},
   "source": [
    "# Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "36bbd38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "Y_train=df.iloc[1:8040]\n",
    "Y_train.drop(['availability', 'Unnamed: 0','bedrooms','total_sqft','bath','balcony','ranked','price in rupees'], axis=1, inplace=True)\n",
    "\n",
    "X_train=df.iloc[1:8040]\n",
    "X_train.drop(['area_type', 'Unnamed: 0'], axis=1, inplace=True)\n",
    "\n",
    "Y_vald=df.iloc[8041:10051]\n",
    "Y_vald.drop(['availability', 'Unnamed: 0','bedrooms','total_sqft','bath','balcony','ranked','price in rupees'], axis=1, inplace=True)\n",
    "\n",
    "X_vald=df.iloc[8041:10051]\n",
    "X_vald.drop(['area_type', 'Unnamed: 0'], axis=1, inplace=True)\n",
    "\n",
    "Y_test=df.iloc[10051:12563]\n",
    "Y_test.drop(['availability', 'Unnamed: 0','bedrooms','total_sqft','bath','balcony','ranked','price in rupees'], axis=1, inplace=True)\n",
    "\n",
    "X_test=df.iloc[10051:12563,2:].values\n",
    "X_test=X_test.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c2cb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken: 00:01:23\n"
     ]
    }
   ],
   "source": [
    "maindf=pd.DataFrame().T\n",
    "\n",
    "start_time = time.time()\n",
    "clf = Adaboost(min_interaction=20)\n",
    "clf.fit(X_vald, Y_vald)\n",
    "Y_pred = clf.predict(X_test)\n",
    "seconds = time.time() - start_time\n",
    "print('Time Taken:', time.strftime(\"%H:%M:%S\",time.gmtime(seconds)))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(Y_test, Y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "accuracy= (tp + tn)/(tp + tn + fp + fn)\n",
    "sensitivity= tp / (tp + fn )\n",
    "\n",
    "df = pd.DataFrame([25,accuracy,sensitivity,specificity]).T\n",
    "df = df.rename(columns={0:'min_interaction',1: 'Accuracy',2:'Sensitivity',3:'Specificity'})\n",
    "\n",
    "maindf = maindf.append(df,ignore_index=True)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "clf = Adaboost(min_interaction=30)\n",
    "clf.fit(X_vald, Y_vald)\n",
    "Y_pred = clf.predict(X_test)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(Y_test, Y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "accuracy= (tp + tn)/(tp + tn + fp + fn)\n",
    "sensitivity= tp / (tp + fn )\n",
    "\n",
    "df = pd.DataFrame([30,accuracy,sensitivity,specificity]).T\n",
    "df = df.rename(columns={0:'min_interaction',1: 'Accuracy',2:'Sensitivity',3:'Specificity'})\n",
    "\n",
    "maindf = maindf.append(df,ignore_index=True)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "clf = Adaboost(min_interaction=40)\n",
    "clf.fit(X_vald, Y_vald)\n",
    "Y_pred = clf.predict(X_test)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(Y_test, Y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "accuracy= (tp + tn)/(tp + tn + fp + fn)\n",
    "sensitivity= tp / (tp + fn )\n",
    "\n",
    "df = pd.DataFrame([40,accuracy,sensitivity,specificity]).T\n",
    "df = df.rename(columns={0:'min_interaction',1: 'Accuracy',2:'Sensitivity',3:'Specificity'})\n",
    "\n",
    "maindf = maindf.append(df,ignore_index=True)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "maindf.style.apply(lambda x: ['background: lightgreen' if x.name in [1] else '' for i in x], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cde202",
   "metadata": {},
   "source": [
    "# <h1 style='color:red'>Section C:</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a494a79",
   "metadata": {},
   "source": [
    "# 1. Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267f016d",
   "metadata": {},
   "source": [
    "<h3>Classification<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ac58b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RunTime: 1.0999679565429688 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Decision Tree Classifier</th>\n",
       "      <td>0.893871</td>\n",
       "      <td>0.486188</td>\n",
       "      <td>0.962492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Accuracy  Sensitivity  Specificity\n",
       "Decision Tree Classifier  0.893871     0.486188     0.962492"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-----------------Classification----------------\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "X_cls = df[['availability','bedrooms','total_sqft','bath','balcony','ranked','price in rupees']]\n",
    "y_cls = df['area_type']\n",
    "# Split dataset into training set and test set\n",
    "X_cls_train, X_cls_test, y_cls_train, y_cls_test = train_test_split(X_cls, y_cls, test_size=0.3, random_state=RSEED)\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=RSEED)\n",
    "\n",
    "# define parameter grid\n",
    "parameters_grid = {\n",
    "    'max_depth': [2, 3],\n",
    "    'min_samples_split': [2, 4],\n",
    "    'min_samples_leaf': [2, 8]\n",
    "}\n",
    "\n",
    "# define grid search\n",
    "grid_search = GridSearchCV(estimator=dt, param_grid=parameters_grid, cv=10)\n",
    "\n",
    "# fit estimator\n",
    "grid_search.fit(X_cls_train, y_cls_train)\n",
    "\n",
    "# get best estimator\n",
    "best = grid_search.best_estimator_\n",
    "\n",
    "# predict\n",
    "y_pred = best.predict(X_cls_test)\n",
    "\n",
    "# calculate accuracy\n",
    "#acc = round(accuracy_score(y_cls_test, y_pred), 3)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_cls_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "accuracy= (tp + tn)/(tp + tn + fp + fn)\n",
    "sensitivity= tp / (tp + fn )\n",
    "\n",
    "df = pd.DataFrame([accuracy,sensitivity,specificity]).T\n",
    "df = df.rename(index={0: 'Decision Tree Classifier'}, columns={0: 'Accuracy',1:'Sensitivity',2:'Specificity'})\n",
    "\n",
    "print(\"---RunTime: %s seconds ---\" % (time.time() - start_time))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06f4fac",
   "metadata": {},
   "source": [
    "<h3> Regression <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "76daabec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "df = pd.get_dummies(df, columns=['area_type'], drop_first=True)\n",
    "df = df.astype({\"balcony\":\"int\",\"availability\":\"int\",\"bedrooms\":\"int\",\"total_sqft\":\"int\",\"bath\":\"int\",\"price in rupees\":\"int\"})\n",
    "\n",
    "Y_train=df.iloc[1:8040]\n",
    "Y_train.drop(['availability', 'Unnamed: 0','bedrooms','total_sqft','bath','balcony','ranked','area_type_P'], axis=1, inplace=True)\n",
    "\n",
    "X_train=df.iloc[1:8040]\n",
    "X_train.drop(['price in rupees', 'Unnamed: 0'], axis=1, inplace=True)\n",
    "\n",
    "Y_vald=df.iloc[8041:10051]\n",
    "Y_vald.drop(['availability', 'Unnamed: 0','bedrooms','total_sqft','bath','balcony','ranked','area_type_P'], axis=1, inplace=True)\n",
    "\n",
    "X_vald=df.iloc[8041:10051]\n",
    "X_vald.drop(['price in rupees', 'Unnamed: 0'], axis=1, inplace=True)\n",
    "\n",
    "Y_test=df.iloc[10051:12563]\n",
    "Y_test.drop(['availability', 'Unnamed: 0','bedrooms','total_sqft','bath','balcony','ranked','area_type_P'], axis=1, inplace=True)\n",
    "\n",
    "X_test=df.iloc[10051:12563]\n",
    "X_test.drop(['price in rupees', 'Unnamed: 0'], axis=1, inplace=True)\n",
    "X_test=df.iloc[10051:12563,1:-1].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "18ecbdb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166043271"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeRegressor(random_state=RSEED)\n",
    "\n",
    "# define parameter grid\n",
    "parameters_grid = {\n",
    "    'max_depth': [2, 3],\n",
    "    'min_samples_split': [2, 4],\n",
    "    'min_samples_leaf': [2, 8]\n",
    "}\n",
    "start_time = time.time()\n",
    "# fit estimator\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# get best estimator\n",
    "best = grid_search.best_estimator_\n",
    "\n",
    "# predict\n",
    "y_pred = best.predict(X_test)\n",
    "print(\"---RunTime: %s seconds ---\" % (time.time() - start_time))\n",
    "# calculate MSE\n",
    "MSE = round(mean_squared_error(Y_test, y_pred), 1)\n",
    "\n",
    "\n",
    "df = pd.DataFrame([abs(MSE)].T\n",
    "df = df.rename(index={0: 'Decision Tree Regressor'}, columns={0: 'MSE'})\n",
    "df['MSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e60102",
   "metadata": {},
   "outputs": [],
   "source": [
    "<h3>\"Adaboost\"<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7b76ec11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AdaBoost Classifier</th>\n",
       "      <td>0.903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Accuracy\n",
       "AdaBoost Classifier     0.903"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-----------------AdaBoost----------------\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "Y_train=df.iloc[1:8040]\n",
    "Y_train.drop(['availability', 'Unnamed: 0','bedrooms','total_sqft','bath','balcony','ranked','price in rupees'], axis=1, inplace=True)\n",
    "\n",
    "X_train=df.iloc[1:8040]\n",
    "X_train.drop(['area_type', 'Unnamed: 0'], axis=1, inplace=True)\n",
    "\n",
    "Y_vald=df.iloc[8041:10051]\n",
    "Y_vald.drop(['availability', 'Unnamed: 0','bedrooms','total_sqft','bath','balcony','ranked','price in rupees'], axis=1, inplace=True)\n",
    "\n",
    "X_vald=df.iloc[8041:10051]\n",
    "X_vald.drop(['area_type', 'Unnamed: 0'], axis=1, inplace=True)\n",
    "\n",
    "Y_test=df.iloc[10051:12563]\n",
    "Y_test.drop(['availability', 'Unnamed: 0','bedrooms','total_sqft','bath','balcony','ranked','price in rupees'], axis=1, inplace=True)\n",
    "\n",
    "X_test=df.iloc[10051:12563,2:].values\n",
    "\n",
    "start_time = time.time()\n",
    "# define model\n",
    "ab = AdaBoostClassifier(random_state=RSEED)\n",
    "\n",
    "# define parameter grid\n",
    "parameters_grid = {\n",
    "    'n_estimators': [20, 50]\n",
    "}\n",
    "\n",
    "# define grid search\n",
    "grid_search = GridSearchCV(estimator=ab, param_grid=parameters_grid, cv=10)\n",
    "\n",
    "grid_search.fit(X_train, Y_train)\n",
    "best = grid_search.best_estimator_\n",
    "# predict\n",
    "y_pred = best.predict(X_test)\n",
    "print(\"---RunTime: %s seconds ---\" % (time.time() - start_time))\n",
    "# calculate accuracy\n",
    "acc = round(accuracy_score(Y_test, y_pred), 3)\n",
    "\n",
    "df = pd.DataFrame([acc]).T\n",
    "df = df.rename(index={0: 'AdaBoost Classifier'}, columns={0: 'Accuracy'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115bda33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bd4cf14",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl; font-size:20px;\"> תשובות לשאלות:\n",
    "\n",
    "1. בעץ סיווג המדד דיוק יצא כמעט זהה בהפרש של 0.01\n",
    "2. יצא לנו מדד שהוא גבוהה יותר מהמדד ב-SKLEARN.\n",
    "ע\"מ לשפר את הביצועים שלנו ראשית אפשר להשתמש בגריד, שייתכן וימצא ערכים טובים יותר מאשר הערכים שאנחנו עושים בבדיקה.\n",
    "עוד סיבה היא שהסיווג שלהם סיווג ערכים מסויימים בעלים אחרים דבר ששינה את הממוצע בעלים,ועלול להוביל לשינוי בהפרשים כשמדובר על הרבה תצפיות.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e3d6d2",
   "metadata": {},
   "source": [
    "<h1 style='color:red'>Section D:</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e1f07f",
   "metadata": {},
   "source": [
    "<h3>Gradient Boost Regressor:<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f5bd45",
   "metadata": {},
   "source": [
    "<h3>2. Classification Metrics: <h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db92c42",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl; font-size:20px;\"> הוספנו sensitivity ו- specificity </div>\n",
    "<div style=\"direction:rtl; font-size:20px;\">אין הבדל משמעותי  specificity </div>\n",
    "<div style=\"direction:rtl; font-size:20px;\">יש הבדל   sensitivity </div>\n",
    "<div style=\"direction:rtl; font-size:20px;\">הסבר הגיוני הוא שיש לנו יותר רשומות שהן P מאשר B\n",
    "ולכן נשנה חלק מערכי ה-B להיות P ע\"י אלגוריתם KNN,NEARMISS וכו'</div>\n",
    "\n",
    "<div style=\"direction:rtl; font-size:20px;\"> בעץ רגרסייה יש לנו ערכים עם הפרש משמעותי מאוד דבר שגורם לסטייה להיות גבוהה .\n",
    "לכן נפעיל לוג על כל ערך ב\"מחיר\" , וזה יצמצם את ההפרשים והסטייה.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddfdcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ניתן להשתמש בפונקציית מערכת שעושה את זה\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE()\n",
    "x_smote, y_smote = smote.fit_resample(x, y)\n",
    "\n",
    "# בעץ רגרסייה\n",
    "df=['price in ruppes']= np.log(df['price in ruppes'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
